{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2dc9aa-92e8-486a-95c2-41c5719dc972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nnunetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26ec9d23-6f91-46f0-9893-7348d02ae4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have to install CUDA to run this code\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print('You have to install CUDA to run this code')\n",
    "else:\n",
    "    print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034ce5a4-cffa-4128-898a-6515440b495a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " /Users/mohannatd/Desktop/test/final\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "\n",
    "os.environ[\"nnUNet_raw\"] = join('.')\n",
    "os.environ[\"nnUNet_preprocessed\"] = join('.')\n",
    "\n",
    "# Put weights path(final directory) here. For example: /Users/mohannatd/Desktop/final\n",
    "os.environ[\"nnUNet_results\"] = input()\n",
    "\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import nibabel as nib\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from torchvision.models.vgg import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed67c70d-5075-44c1-bca9-543b6ed917de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(part, whole):\n",
    "  Percentage = 100 * part/whole\n",
    "  return str(round(Percentage) + 1) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d580b93b-2c78-4cc9-8cb5-cd6c43666967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(torch.nn.Module):\n",
    "    def __init__(self, device='cuda'):\n",
    "        super(VGG19, self).__init__()\n",
    "        features = list(vgg19(pretrained=True).features)\n",
    "        if device == \"cuda\":\n",
    "            self.features = nn.ModuleList(features).cuda().eval()\n",
    "        else:\n",
    "            self.features = nn.ModuleList(features).eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_maps = []\n",
    "        for idx, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            if idx == 3:\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n",
    "\n",
    "class Fusion:\n",
    "    def __init__(self, input):\n",
    "        \"\"\"\n",
    "        Class Fusion constructor\n",
    "\n",
    "        Instance Variables:\n",
    "            self.images: input images\n",
    "            self.model: CNN model, default=vgg19\n",
    "            self.device: either 'cuda' or 'cpu'\n",
    "        \"\"\"\n",
    "        self.input_images = input\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = VGG19(self.device)\n",
    "\n",
    "    def fuse(self):\n",
    "        \"\"\"\n",
    "        A top level method which fuse self.images\n",
    "        \"\"\"\n",
    "        # Convert all images to YCbCr format\n",
    "        self.normalized_images = [-1 for img in self.input_images]\n",
    "        self.YCbCr_images = [-1 for img in self.input_images]\n",
    "        for idx, img in enumerate(self.input_images):\n",
    "            if not self._is_gray(img):\n",
    "                self.YCbCr_images[idx] = self._RGB_to_YCbCr(img)\n",
    "                self.normalized_images[idx] = self.YCbCr_images[idx][:, :, 0]\n",
    "            else:\n",
    "                self.normalized_images[idx] = img / 255.\n",
    "        # Transfer all images to PyTorch tensors\n",
    "        self._tranfer_to_tensor()\n",
    "        # Perform fuse strategy\n",
    "        fused_img = self._fuse()[:, :, 0]\n",
    "        # Reconstruct fused image given rgb input images\n",
    "        for idx, img in enumerate(self.input_images):\n",
    "            if not self._is_gray(img):\n",
    "                self.YCbCr_images[idx][:, :, 0] = fused_img\n",
    "                fused_img = self._YCbCr_to_RGB(self.YCbCr_images[idx])\n",
    "                fused_img = np.clip(fused_img, 0, 1)\n",
    "\n",
    "        return (fused_img * 255).astype(np.uint8)\n",
    "\n",
    "    def _fuse(self):\n",
    "        \"\"\"\n",
    "        Perform fusion algorithm\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "\n",
    "            imgs_sum_maps = [-1 for tensor_img in self.images_to_tensors]\n",
    "            for idx, tensor_img in enumerate(self.images_to_tensors):\n",
    "                imgs_sum_maps[idx] = []\n",
    "                feature_maps = self.model(tensor_img)\n",
    "                for feature_map in feature_maps:\n",
    "                    sum_map = torch.sum(feature_map, dim=1, keepdim=True)\n",
    "                    imgs_sum_maps[idx].append(sum_map)\n",
    "\n",
    "            max_fusion = None\n",
    "            for sum_maps in zip(*imgs_sum_maps):\n",
    "                features = torch.cat(sum_maps, dim=1)\n",
    "                weights = self._softmax(F.interpolate(features,\n",
    "                                        size=self.images_to_tensors[0].shape[2:]))\n",
    "                weights = F.interpolate(weights,\n",
    "                                        size=self.images_to_tensors[0].shape[2:])\n",
    "                current_fusion = torch.zeros(self.images_to_tensors[0].shape)\n",
    "                for idx, tensor_img in enumerate(self.images_to_tensors):\n",
    "                    current_fusion += tensor_img * weights[:,idx]\n",
    "                if max_fusion is None:\n",
    "                    max_fusion = current_fusion\n",
    "                else:\n",
    "                    max_fusion = torch.max(max_fusion, current_fusion)\n",
    "\n",
    "            output = np.squeeze(max_fusion.cpu().numpy())\n",
    "            if output.ndim == 3:\n",
    "                output = np.transpose(output, (1, 2, 0))\n",
    "            return output\n",
    "\n",
    "    def _RGB_to_YCbCr(self, img_RGB):\n",
    "        \"\"\"\n",
    "        A private method which converts an RGB image to YCrCb format\n",
    "        \"\"\"\n",
    "        img_RGB = img_RGB.astype(np.float32) / 255.\n",
    "        return cv2.cvtColor(img_RGB, cv2.COLOR_RGB2YCrCb)\n",
    "\n",
    "    def _YCbCr_to_RGB(self, img_YCbCr):\n",
    "        \"\"\"\n",
    "        A private method which converts a YCrCb image to RGB format\n",
    "        \"\"\"\n",
    "        img_YCbCr = img_YCbCr.astype(np.float32)\n",
    "        return cv2.cvtColor(img_YCbCr, cv2.COLOR_YCrCb2RGB)\n",
    "\n",
    "    def _is_gray(self, img):\n",
    "        \"\"\"\n",
    "        A private method which returns True if image is gray, otherwise False\n",
    "        \"\"\"\n",
    "        if len(img.shape) < 3:\n",
    "            return True\n",
    "        if img.shape[2] == 1:\n",
    "            return True\n",
    "        b, g, r = img[:,:,0], img[:,:,1], img[:,:,2]\n",
    "        if (b == g).all() and (b == r).all():\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _softmax(self, tensor):\n",
    "        \"\"\"\n",
    "        A private method which compute softmax ouput of a given tensor\n",
    "        \"\"\"\n",
    "        tensor = torch.exp(tensor)\n",
    "        tensor = tensor / tensor.sum(dim=1, keepdim=True)\n",
    "        return tensor\n",
    "\n",
    "    def _tranfer_to_tensor(self):\n",
    "        \"\"\"\n",
    "        A private method to transfer all input images to PyTorch tensors\n",
    "        \"\"\"\n",
    "        self.images_to_tensors = []\n",
    "        for image in self.normalized_images:\n",
    "            np_input = image.astype(np.float32)\n",
    "            if np_input.ndim == 2:\n",
    "                np_input = np.repeat(np_input[None, None], 3, axis=1)\n",
    "            else:\n",
    "                np_input = np.transpose(np_input, (2, 0, 1))[None]\n",
    "            if self.device == \"cuda\":\n",
    "                self.images_to_tensors.append(torch.from_numpy(np_input).cuda())\n",
    "            else:\n",
    "                self.images_to_tensors.append(torch.from_numpy(np_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef296bc-255c-4fe7-a5ac-f7f42ba0ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_learning_fusion(input_images):\n",
    "  FU = Fusion(input_images)\n",
    "  fusion_img = FU.fuse()\n",
    "  return fusion_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e073ecf1-bd2f-4945-8b48-be07a5da2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(image):\n",
    "    img_min = np.min(image)\n",
    "    img_max = np.max(image)\n",
    "    image = (image - img_min ) / ( img_max - img_min)\n",
    "    image = image * 255\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c378a3b-d85f-4ec9-9565-ecf685cc9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion(raw_data_path):\n",
    "    base_dir = raw_data_path\n",
    "\n",
    "    fusion_modes = ['t1n-t1c', 't1c-t2f', 't1c-t2w']\n",
    "    images = []\n",
    "    done = []\n",
    "    for i, mode in enumerate(fusion_modes):\n",
    "        Final_fusion3D = ''\n",
    "        images = [nib.load(join(base_dir, '{}.nii.gz'.format(mode.split('-')[0]))).get_fdata()]\n",
    "        images.append(nib.load(join(base_dir, '{}.nii.gz'.format(mode.split('-')[1]))).get_fdata())\n",
    "        for slice_num in range(155):\n",
    "            if (slice_num%10) == 0:\n",
    "              clear_output()\n",
    "              print(percentage(155*i + slice_num + 1, 155*3))\n",
    "              print('Wait until the process ends')\n",
    "              for do in done:\n",
    "                  print(do + ': Done')\n",
    "            slices = [normalizer(image[:,:,slice_num]) for image in images]\n",
    "            fusion = zero_learning_fusion(slices)\n",
    "            if slice_num == 0:\n",
    "                Final_fusion3D = fusion\n",
    "            else:\n",
    "                Final_fusion3D = np.dstack((Final_fusion3D ,fusion))\n",
    "    \n",
    "        output = nib.Nifti1Image(Final_fusion3D, np.eye(4))\n",
    "        output.header.get_xyzt_units()\n",
    "        output.to_filename(join(base_dir, '{}.nii.gz'.format(mode)))\n",
    "        done.append(mode)\n",
    "        if mode == 't1c-t2w':\n",
    "            print(mode + ': Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b329dd9-114c-4f04-bf2c-93ac94686cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " /Users/mohannatd/Desktop/inp\n"
     ]
    }
   ],
   "source": [
    "'''Put raw input path with the following names here.\n",
    "For example: /Users/mohannatd/Desktop/raw_input\n",
    "\n",
    "/Users/mohannatd/Desktop/raw_input :\n",
    "|__ t1n.nii.gz\n",
    "|__ t1c.nii.gz\n",
    "|__ t2f.nii.gz\n",
    "|__ t2w.nii.gz\n",
    "\n",
    "'''\n",
    "input_path = input()\n",
    "\n",
    "fusion(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd5730-1971-4bb9-866b-9aab52e8ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagestr = join('.', 'input')\n",
    "\n",
    "try:\n",
    "    os.mkdir(imagestr)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "shutil.copy(join(input_path, \"t1n-t1c.nii.gz\"), join(imagestr, 'Brain_0000.nii.gz'))\n",
    "shutil.copy(join(input_path, \"t2f.nii.gz\"), join(imagestr, 'Brain_0001.nii.gz'))\n",
    "shutil.copy(join(input_path, \"t1c-t2w.nii.gz\"), join(imagestr, 'Brain_0002.nii.gz'))\n",
    "shutil.copy(join(input_path, \"t1c-t2f.nii.gz\"), join(imagestr, 'Brain_0003.nii.gz'))\n",
    "\n",
    "output_path = join(input_path, 'output')\n",
    "\n",
    "!nnUNetv2_predict -i $imagestr -o $output_path -d 13 -c 3d_fullres -f all -tr nnUNetTrainerDiceCELoss_noSmooth\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
